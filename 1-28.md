| terminology |pronunciation  |
| --- | --- |
| feed forward layer |  |
| probability distribution |  |
| rectified linear unit |  |
| non-linearity[ˌlɪniˈærəti] |  |
| dense low dimensional vector |  |
| perplexity | /pɚ'plɛksəti/ |
| a training set
a evaluation set/ɪˌvæljuˈeɪʃn/ |  |
| recurrent neural network/rɪˈkɜːrənt/ |  |
| matrices  / metric |  |
| terminology |  |
| parameter |  |
|attention score|which part you should pay attention to|
|representational capacity| |
|weighted average||
|parallelize / parallelism / parallel||
| back propagation algorithm |  |
| --- | --- |
| loss function
cross entropy
square loss |  |
| stochastic gradient descent | /stə'kæstɪk/ |
| learning rate
fixed / dynamic | control the step size |
| convergence |  |
| feed forward layer |  |
| probability distribution |  |
| rectified linear unit |  |
| non-linearity[ˌlɪniˈærəti] |  |
| dense low dimensional vector |  |
| perplexity | /pɚ'plɛksəti/ |
| a training set
a evaluation set/ɪˌvæljuˈeɪʃn/ |  |
| recurrent neural network/rɪˈkɜːrənt/ |  |
| matrices  / metric |  |
| terminology |  |
| parameter
hyper parameters |  |
| initialize |  |
| derivatives
partial | /dɪˈrɪvətɪv/ |
| negative gradient |  负梯度 |
| concatenate |  v. |
| smooth and differentiable | /ˌdɪfə'rɛnʃəbəl/ |
| maximize/ˈmæksɪˌmaɪz/
maximum/ˈmæksɪməm/
minimize/ˈmɪnɪˌmaɪz/
minimum/ˈmɪnɪməm/ |  |
| maximum likelihood |  |
| optimization / optimize / optimizer |  |
| non-convex |  |
| subtract
multiply
minus |  |
| second order terms |  二阶项 |
| accumulate |  |
| conditional probability |  |
| discrete / continuous |  |
| chain rule |  |
| semantic |  |
| syntactic |  |
| parallelize / parallelism / parallel | /'pærəlelaɪz/ |
| mechanism |  |
| attention score |  |
| infinity |  |
| weighted average |  |
| dimensionality |  |
| representations |  |
| convolutional |  |
| residual |  |
| scaling law |  |
